{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8ae800-4a53-4a04-a123-bebaa477cdf2",
   "metadata": {},
   "source": [
    "# Comparison Of PIELM and PINNs\n",
    "####\n",
    "## For Formulation B\n",
    "###\n",
    "#### For flat plate (m=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cc4cc-38da-41d7-a937-d87c213794c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import trapz  # Use trapz for integration\n",
    "\n",
    "# Function for PINN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = self.linears[i](x)\n",
    "            x = self.activation(z)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1, y1 = x[[0]], y[[0]]\n",
    "        g1 = x1.clone().requires_grad_(True)\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2, y2 = x[[1]], y[[1]]\n",
    "        g2 = x2.clone().requires_grad_(True)\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone().requires_grad_(True)\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "\n",
    "        pde = f_eta3 + ((m + 1) / 2) * f3 * f_eta2 + m * (1 - (f_eta) ** 2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        return loss    \n",
    "\n",
    "# Function for PIELM\n",
    "class PIELM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        z = self.fc1(x)\n",
    "        a = self.activation(z)\n",
    "        return self.fc2(a)\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1, y1 = x[[0]], y[[0]]\n",
    "        g1 = x1.clone().requires_grad_(True)\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2, y2 = x[[1]], y[[1]]\n",
    "        g2 = x2.clone().requires_grad_(True)\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone().requires_grad_(True)\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + ((m + 1) / 2) * f3 * f_eta2 + m * (1 - (f_eta) ** 2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())  # Store total loss\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        return loss\n",
    "\n",
    "# Initialization and training for PINN\n",
    "np.random.seed(941)\n",
    "torch.manual_seed(11)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "m = 0\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0], [1]])\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "layers = [1, 20, 1]\n",
    "pinn = NN(layers)\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters(), lr=0.1, max_iter=5000)\n",
    "optimizer.step(pinn.closure)\n",
    "\n",
    "# Initialization and training for PIELM\n",
    "hidden_size = 50\n",
    "pielm = PIELM()\n",
    "optimizer = torch.optim.LBFGS(pielm.parameters(), lr=0.005, max_iter=20000)\n",
    "optimizer.step(pielm.closure)\n",
    "\n",
    "# Plot total loss vs iterations for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(1, len(pinn.Ltotal) + 1), np.log(pinn.Ltotal), marker='o', label='PINN Total Loss')\n",
    "plt.plot(np.arange(1, len(pielm.Ltotal) + 1), np.log(pielm.Ltotal), marker='x', label='PIELM Total Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ln(Total Loss)')\n",
    "plt.title('Total Loss vs Iteration Comparison')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95d52e0-73ff-4d3e-8e2a-cbf36f7cc03e",
   "metadata": {},
   "source": [
    "#### For Stagnation point flow (m=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7970a88c-4867-4127-8970-76ef9614c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import trapz  # Use trapz for integration\n",
    "\n",
    "# Function for PINN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i + 1]) for i in range(len(layers) - 1)])\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = self.linears[i](x)\n",
    "            x = self.activation(z)\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1, y1 = x[[0]], y[[0]]\n",
    "        g1 = x1.clone().requires_grad_(True)\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2, y2 = x[[1]], y[[1]]\n",
    "        g2 = x2.clone().requires_grad_(True)\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone().requires_grad_(True)\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "\n",
    "        pde = f_eta3 + ((m + 1) / 2) * f3 * f_eta2 + m * (1 - (f_eta) ** 2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        return loss    \n",
    "\n",
    "# Function for PIELM\n",
    "class PIELM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        z = self.fc1(x)\n",
    "        a = self.activation(z)\n",
    "        return self.fc2(a)\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1, y1 = x[[0]], y[[0]]\n",
    "        g1 = x1.clone().requires_grad_(True)\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2, y2 = x[[1]], y[[1]]\n",
    "        g2 = x2.clone().requires_grad_(True)\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone().requires_grad_(True)\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + ((m + 1) / 2) * f3 * f_eta2 + m * (1 - (f_eta) ** 2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())  # Store total loss\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        return loss\n",
    "\n",
    "# Initialization and training for PINN\n",
    "np.random.seed(941)\n",
    "torch.manual_seed(11)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "m = 1\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0], [1]])\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "layers = [1, 20, 1]\n",
    "pinn = NN(layers)\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters(), lr=0.1, max_iter=5000)\n",
    "optimizer.step(pinn.closure)\n",
    "\n",
    "# Initialization and training for PIELM\n",
    "hidden_size = 50\n",
    "pielm = PIELM()\n",
    "optimizer = torch.optim.LBFGS(pielm.parameters(), lr=0.005, max_iter=20000)\n",
    "optimizer.step(pielm.closure)\n",
    "\n",
    "# Plot total loss vs iterations for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.arange(1, len(pinn.Ltotal) + 1), np.log(pinn.Ltotal), marker='o', label='PINN Total Loss')\n",
    "plt.plot(np.arange(1, len(pielm.Ltotal) + 1), np.log(pielm.Ltotal), marker='x', label='PIELM Total Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ln(Total Loss)')\n",
    "plt.title('Total Loss vs Iteration Comparison')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab989e28-9a42-471e-b7d5-fcccfc1239be",
   "metadata": {},
   "source": [
    "#### We can see that from the above comparison plot PIELM converges early as compared to PINNs but f' and f'' plots have slight inaccurate represntation through PIELM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8548c0-00a2-4eeb-8032-53bb085cae15",
   "metadata": {},
   "source": [
    "### \n",
    "## For Formulation A\n",
    "######\n",
    "#### For Flat plate (beta=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde8ad97-a0b9-4024-9c2d-0bdb9a63bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First Implementation (PINN)\n",
    "# Creation of 1D array (representing data sampling)\n",
    "np.random.seed(941)\n",
    "torch.manual_seed(11)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "\n",
    "# PDE parameters\n",
    "beta = 0  # Set beta = 0 for flat plate case\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0],[1]])\n",
    "\n",
    "# Converting numpy arrays into tensors\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = self.linears[i](x)\n",
    "            x = self.activation(z)\n",
    "        a = self.linears[-1](x)\n",
    "        return a\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1 = x[[0]]\n",
    "        y1 = y[[0]]\n",
    "        g1 = x1.clone()\n",
    "        g1.requires_grad = True\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2 = x[[1]]\n",
    "        y2 = y[[1]]\n",
    "        g2 = x2.clone()\n",
    "        g2.requires_grad = True\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "    \n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone()\n",
    "        g3.requires_grad = True\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + f3 * f_eta2 + beta * (1 - (f_eta)**2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())\n",
    "        return total_loss\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(f'Iteration {self.iter}: Loss = {loss.item()}')\n",
    "        return loss    \n",
    "    \n",
    "# Specification of network architecture\n",
    "layers = [1, 20, 1]\n",
    "pinn = NN(layers)\n",
    "max_iterations = 5000\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters(), lr=0.1, max_iter=max_iterations,\n",
    "                              tolerance_grad=1e-7, tolerance_change=1e-10,\n",
    "                              history_size=500, line_search_fn='strong_wolfe')\n",
    "\n",
    "optimizer.step(pinn.closure)\n",
    "print(f'Total iterations: {pinn.iter}')\n",
    "\n",
    "# Second Implementation (PIELM)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "beta = 0.0\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0], [1]])\n",
    "\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "class PIELM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        z = self.fc1(x)\n",
    "        a = self.activation(z)\n",
    "        f = self.fc2(a)\n",
    "        return f\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1 = x[[0]]\n",
    "        y1 = y[[0]]\n",
    "        g1 = x1.clone()\n",
    "        g1.requires_grad = True\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2 = x[[1]]\n",
    "        y2 = y[[1]]\n",
    "        g2 = x2.clone()\n",
    "        g2.requires_grad = True\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone()\n",
    "        g3.requires_grad = True\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + f3 * f_eta2 + beta * (1 - f_eta**2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = loss1 + loss2  # Total loss for PIELM\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        return loss    \n",
    "\n",
    "# Hidden size for PIELM\n",
    "hidden_size = 20\n",
    "pielm = PIELM()\n",
    "optimizer = torch.optim.LBFGS(pielm.parameters(), lr=0.1, max_iter=5000,\n",
    "                               tolerance_grad=1e-7, tolerance_change=1e-10)\n",
    "\n",
    "optimizer.step(pielm.closure)\n",
    "\n",
    "# Comparison of Total Losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pinn.Ltotal, label='PINN Total Loss',marker='o', color='blue')\n",
    "plt.plot(pielm.Lbc, label='PIELM Total Loss', marker='x', color='orange')  # Change to total loss for PIELM\n",
    "plt.yscale('log')\n",
    "plt.title('Comparison of Total Losses')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3ac4f-63af-43e7-b93b-d4aa89e44285",
   "metadata": {},
   "source": [
    "#### For Stagnation point flow (beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f5088-32d8-46e2-a292-c0e32074b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# First Implementation (PINN)\n",
    "# Creation of 1D array (representing data sampling)\n",
    "np.random.seed(941)\n",
    "torch.manual_seed(11)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "\n",
    "# PDE parameters\n",
    "beta = 0  # Set beta = 0 for flat plate case\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0],[1]])\n",
    "\n",
    "# Converting numpy arrays into tensors\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "        self.Ltotal = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        for i in range(len(layers) - 2):\n",
    "            z = self.linears[i](x)\n",
    "            x = self.activation(z)\n",
    "        a = self.linears[-1](x)\n",
    "        return a\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1 = x[[0]]\n",
    "        y1 = y[[0]]\n",
    "        g1 = x1.clone()\n",
    "        g1.requires_grad = True\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2 = x[[1]]\n",
    "        y2 = y[[1]]\n",
    "        g2 = x2.clone()\n",
    "        g2.requires_grad = True\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "    \n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone()\n",
    "        g3.requires_grad = True\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + f3 * f_eta2 + beta * (1 - (f_eta)**2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = abs(loss1) + abs(loss2)\n",
    "        self.Ltotal.append(total_loss.item())\n",
    "        return total_loss\n",
    "    \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(f'Iteration {self.iter}: Loss = {loss.item()}')\n",
    "        return loss    \n",
    "    \n",
    "# Specification of network architecture\n",
    "layers = [1, 20, 1]\n",
    "pinn = NN(layers)\n",
    "max_iterations = 5000\n",
    "optimizer = torch.optim.LBFGS(pinn.parameters(), lr=0.1, max_iter=max_iterations,\n",
    "                              tolerance_grad=1e-7, tolerance_change=1e-10,\n",
    "                              history_size=500, line_search_fn='strong_wolfe')\n",
    "\n",
    "optimizer.step(pinn.closure)\n",
    "print(f'Total iterations: {pinn.iter}')\n",
    "\n",
    "# Second Implementation (PIELM)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "eta = np.linspace(0, 6, 1000)[:, None]\n",
    "eta_min = eta[[0]]\n",
    "eta_max = eta[[-1]]\n",
    "beta = 1\n",
    "bc = eta[[0, -1]]\n",
    "f_bc = np.array([[0], [1]])\n",
    "\n",
    "eta = torch.from_numpy(eta).float()\n",
    "eta_min = torch.from_numpy(eta_min).float()\n",
    "eta_max = torch.from_numpy(eta_max).float()\n",
    "bc = torch.from_numpy(bc).float()\n",
    "f_bc = torch.from_numpy(f_bc).float()\n",
    "\n",
    "class PIELM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction='mean')\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.iter = 0\n",
    "        self.Lbc = []\n",
    "        self.Lpde = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = (x - eta_min) / (eta_max - eta_min)\n",
    "        z = self.fc1(x)\n",
    "        a = self.activation(z)\n",
    "        f = self.fc2(a)\n",
    "        return f\n",
    "\n",
    "    def bc_loss(self, x, y):\n",
    "        x1 = x[[0]]\n",
    "        y1 = y[[0]]\n",
    "        g1 = x1.clone()\n",
    "        g1.requires_grad = True\n",
    "        f1 = self.forward(g1)\n",
    "        f_etab1 = autograd.grad(f1, g1, torch.ones([x1.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        x2 = x[[1]]\n",
    "        y2 = y[[1]]\n",
    "        g2 = x2.clone()\n",
    "        g2.requires_grad = True\n",
    "        f2 = self.forward(g2)\n",
    "        f_etab2 = autograd.grad(f2, g2, torch.ones([x2.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        loss_bc1 = self.loss_function(f1, y1) + self.loss_function(f_etab1, torch.zeros_like(f_etab1))\n",
    "        loss_bc2 = self.loss_function(f_etab2, y2)\n",
    "        loss_bc = loss_bc1 + loss_bc2\n",
    "        self.Lbc.append(loss_bc.item())\n",
    "        return loss_bc\n",
    "\n",
    "    def PDE(self, x):\n",
    "        g3 = x.clone()\n",
    "        g3.requires_grad = True\n",
    "        f3 = self.forward(g3)\n",
    "        f_eta = autograd.grad(f3, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta2 = autograd.grad(f_eta, g3, torch.ones([g3.shape[0], 1]), retain_graph=True, create_graph=True)[0]\n",
    "        f_eta3 = autograd.grad(f_eta2, g3, torch.ones([g3.shape[0], 1]), create_graph=True)[0]\n",
    "        pde = f_eta3 + f3 * f_eta2 + beta * (1 - f_eta**2)\n",
    "        loss_pde = self.loss_function(pde, torch.zeros_like(pde))\n",
    "        self.Lpde.append(loss_pde.item())\n",
    "        return loss_pde\n",
    "\n",
    "    def loss(self, x, xb, yb):\n",
    "        loss1 = self.bc_loss(xb, yb)\n",
    "        loss2 = self.PDE(x)\n",
    "        total_loss = loss1 + loss2  # Total loss for PIELM\n",
    "        return total_loss\n",
    "\n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(eta, bc, f_bc)\n",
    "        loss.backward()\n",
    "        return loss    \n",
    "\n",
    "# Hidden size for PIELM\n",
    "hidden_size = 20\n",
    "pielm = PIELM()\n",
    "optimizer = torch.optim.LBFGS(pielm.parameters(), lr=0.1, max_iter=5000,\n",
    "                               tolerance_grad=1e-7, tolerance_change=1e-10)\n",
    "\n",
    "optimizer.step(pielm.closure)\n",
    "\n",
    "# Comparison of Total Losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pinn.Ltotal, label='PINN Total Loss',marker='o', color='blue')\n",
    "plt.plot(pielm.Lbc, label='PIELM Total Loss', marker='x', color='orange')  # Change to total loss for PIELM\n",
    "plt.yscale('log')\n",
    "plt.title('Comparison of Total Losses')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (Log Scale)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96290f96-1edc-4923-a379-50108cdd688d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
